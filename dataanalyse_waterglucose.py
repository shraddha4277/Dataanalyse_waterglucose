# -*- coding: utf-8 -*-
"""dataanalyse_waterglucose.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EJ-SdRZydB6S5eqCE7K2c0RusiOJY8Fw
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr, ttest_ind
from scipy.signal import savgol_filter
from sklearn.decomposition import PCA

from google.colab import drive
drive.mount('/content/drive')

def processCsv(filepath):
  """
    Reads a CSV file and returns a pandas DataFrame.

    Args:
        filepath (str): The path to the CSV file.

    Returns:
        DataFrame: The loaded dataset as a pandas DataFrame.
    """
  try:
    df = pd.read_csv(filepath)
  except FileNotFoundError:
    print(f"File not found: {filepath}")
    return None
  except Exception as e:
    print(f"Error reading CSV file: {filepath}")
    print(e)
    return None
  return df

def getWavelengthRange():
    """
    Prompt the user to input the desired wavelength range for segmentation.

    Returns:
        tuple: A tuple containing the lower and upper bounds of the wavelength range.
    """
    while True:
        try:
            lower_bound = float(input("Enter the lower bound of the wavelength range: "))
            upper_bound = float(input("Enter the upper bound of the wavelength range: "))
            print("Lower bound:", lower_bound)
            print("Upper bound:", upper_bound)
            if lower_bound >= upper_bound:
                print("Lower bound must be less than the upper bound. Please try again.")
            else:
                return lower_bound, upper_bound
        except ValueError:
            print("Invalid input. Please enter numeric values.")

def prepareDataset(df, filepath):
    """
    Cleans and preprocesses the dataset by renaming columns, filtering rows, and converting data types.

    Args:
        df (DataFrame): The raw dataset.
        filepath (str): The path to the CSV file (used for error logging).

    Returns:
        DataFrame: A cleaned and preprocessed dataset.
    """
    print("Data cleaning....")
    column_names = list(df.iloc[0])  # Extract column names from the first row
    column_names[0] = "Wavelength (nm)"  # Rename the first column for clarity
    filtered_data = df[1:].copy()  # Exclude the first row used for column names
    filtered_data.columns = column_names  # Assign column names

    # Convert "Wavelength (nm)" to numeric
    try:
        filtered_data["Wavelength (nm)"] = pd.to_numeric(filtered_data["Wavelength (nm)"], errors='coerce')
    except Exception as e:
        print(f"Error converting 'Wavelength (nm)' to numeric in file: {filepath}")
        print(e)
        return None

    # Drop rows with invalid "Wavelength (nm)" values
    filtered_data = filtered_data.dropna(subset=["Wavelength (nm)"])

    # Get wavelength range from the user
    print("Please specify the wavelength range for segmentation.")
    lower_bound, upper_bound = getWavelengthRange()

    # Filter data within the user-specified range
    filtered_data = filtered_data[(filtered_data["Wavelength (nm)"] >= lower_bound) &
                                  (filtered_data["Wavelength (nm)"] <= upper_bound)]

    # Convert absorption columns to numeric
    valid_columns = ["Wavelength (nm)"]  # Keep track of valid columns
    for col in filtered_data.columns[1:]:
        try:
            filtered_data[col] = pd.to_numeric(filtered_data[col], errors='coerce')
            valid_columns.append(col)
        except Exception as e:
            print(f"Skipping invalid column '{col}' in file: {filepath}")
            print(e)

    # Keep only valid columns
    filtered_data = filtered_data[valid_columns]
    print("Data is prepared now!")
    return filtered_data

def statForDataset(df):
    """
    Computes and displays statistical summaries for the dataset.

    Args:
        df (DataFrame): The dataset to analyze.
    """
    print("Dataset Statistics:")
    # Compute basic statistics for all columns except the wavelength
    data_no_wavelength = df.iloc[:, 1:]  # Exclude the first column (wavelength)

    # Compute statistics
    statistics = {
    "Mean": data_no_wavelength.mean(),
    "Median": data_no_wavelength.median(),
    "Standard Deviation": data_no_wavelength.std(),
    "Min": data_no_wavelength.min(),
    "Max": data_no_wavelength.max(),
    "Range": data_no_wavelength.max() - data_no_wavelength.min(),
    }

    # Print each statistic
    for stat_name, stat_values in statistics.items():
        print(f"\n{stat_name}:")
        print(stat_values.to_string(index=True))

    # Plotting
    plt.figure(figsize=(15, 8))  # Increased figure size

    # Create subplots for each statistic
    for i, stat_name in enumerate(statistics, 1):
        plt.subplot(3, 3, i)
        statistics[stat_name].plot(kind='bar',
                                   color='skyblue',
                                   edgecolor='black')
        plt.title(f'{stat_name} Across Columns', fontsize=12)
        plt.xticks(rotation=45, ha='right', fontsize=8)
        plt.ylabel(stat_name, fontsize=10)
        plt.grid(axis='y', linestyle='--', alpha=0.7)

    plt.suptitle('Statistical Measures Comparison', fontsize=16)
    plt.tight_layout()
    plt.show()

def visualizeTrends(df):
    """
    Plots the absorption trends for each column in the dataset.

    Args:
        df (DataFrame): The dataset containing wavelength and absorption data.
    """
    plt.figure(figsize=(15, 8))
    for col in df.columns[1:]:
        plt.plot(df["Wavelength (nm)"], df[col], label=col)
    plt.title("Absorption Trends")
    plt.xlabel("Wavelength (nm)")
    plt.ylabel("Absorption")
    plt.legend()
    plt.grid()
    plt.show()

def patternDetection(df):
    """
    Analyzes patterns using monotonicity (Spearman correlation) and visualizes inter-column correlations.

    Args:
        df (DataFrame): The dataset for analysis.
    """
    print("Monotonicity and Correlation Analysis:")

    # Compute monotonicity using Spearman's rank correlation
    monotonicity_results = {}
    for col in df.columns[1:]:
        monotonicity, _ = spearmanr(df["Wavelength (nm)"], df[col])
        monotonicity_results[col] = monotonicity
        print(f"Monotonicity (Spearman's rho) for {col}: {monotonicity:.2f}")

    # Plot correlation heatmap
    corr_df = df.iloc[:, 1:] # drop the 'Wavelength (nm)'
    correlations = corr_df.corr(method='spearman')
    plt.figure(figsize=(15, 8))
    sns.heatmap(correlations, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
    plt.title("Correlation Heatmap")
    plt.show()

def dataSmoothing(df):
    """
    Applies Savitzky-Golay filtering to smooth absorption data.

    Args:
        df (DataFrame): The dataset to smooth.

    Returns:
        DataFrame: The smoothed dataset.
    """
    smoothed_data = df.copy()
    # Exclude the first column from smoothing
    for col in smoothed_data.columns[1:]:
        smoothed_data[col] = savgol_filter(smoothed_data[col], window_length=11, polyorder=3)
    if len(smoothed_data.columns) > 1:
        second_column_name = smoothed_data.columns[1]
        filtered_data = smoothed_data.drop(columns=[second_column_name])
        print(f"Dropped second column: {second_column_name}")
    visualizeTrends(filtered_data)
    return smoothed_data

def testRelationBetweenBlanksAndSamples(df):
    """
    Performs t-tests to compare blanks and samples for statistical differences.

    Args:
        df (DataFrame): The dataset containing blanks and sample data.
    """

    # Explicitly identify blank and sample columns based on known naming patterns
    blank_columns = [col for col in df.columns if col.startswith("blank")]
    sample_columns = [col for col in df.columns if col.endswith("mg")]

    if not blank_columns or not sample_columns:
        print("No blank or sample columns found in the dataset.")
        return

    for blank_col in blank_columns:
        for sample_col in sample_columns:
            try:
                blank_data = df[blank_col].dropna()
                sample_data = df[sample_col].dropna()

                if blank_data.empty or sample_data.empty:
                    print(f"No data to compare between {blank_col} and {sample_col}.")
                    continue

                stat, p_value = ttest_ind(blank_data, sample_data)
                print(f"T-test between {blank_col} and {sample_col}: p-value = {p_value: .4f}")
            except Exception as e:
                print(f"Error performing t-test between {blank_col} and {sample_col}: {e}")

def applyPCA(df):
    """
    Applies Principal Component Analysis (PCA) to reduce dimensionality and visualize data in 2D.

    Args:
        df (DataFrame): The dataset for PCA.

    Displays:
        Scatter plot of the first two principal components.
    """
    print("Applying PCA...")
    pca = PCA(n_components=2)
    absorption_data = df.iloc[:, 1:].values
    principal_components = pca.fit_transform(absorption_data)
    pca_df = pd.DataFrame(principal_components, columns=["PC1", "PC2"])
    print("PCA applied!And we are getting this scatter graph.")
    plt.figure(figsize=(12, 8))
    plt.scatter(pca_df["PC1"], pca_df["PC2"])
    plt.title("PCA of Absorption Data")
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.grid()
    plt.show()

def getValidSegmentCount():
    """
    Prompt user to enter the number of segments with input validation.

    Returns:
        int: Number of segments to process
    """
    while True:
        try:
            segment_count = int(input("How many segments would you like to analyze? (1-5): "))

            # Validate segment count
            if 1 <= segment_count <= 5:
                return segment_count
            else:
                print("Please enter a number between 1 and 5.")

        except ValueError:
            print("Invalid input. Please enter a valid integer.")

if __name__ == "__main__":
    filepath = '/content/drive/MyDrive/gluco_data/set01.csv'
    df = processCsv(filepath)

    if df is not None:
        # Get number of segments from user
        num_segments = getValidSegmentCount()

        # Lists to store segment data
        segments = []
        smoothed_segments = []

        # Process each segment
        for i in range(num_segments):
            print(f"\nEnter details for segment {i+1}:")
            current_segment = prepareDataset(df, filepath)

            if current_segment is not None:
                segments.append(current_segment)

                # Visualization
                print(f"Visualizing segment {i+1}...")
                visualizeTrends(current_segment)

                # Statistical Analysis
                print(f"Statistical info on segment {i+1}...")
                statForDataset(current_segment)

                # Pattern Detection
                print(f"Applying pattern detection on segment {i+1}...")
                patternDetection(current_segment)

                # Smoothing
                print(f"Smoothing segment {i+1}...")
                smoothed_segment = dataSmoothing(current_segment)
                smoothed_segments.append(smoothed_segment)

                # Re-apply Pattern Detection on Smoothed Data
                print(f"Re-applying pattern detection on smoothed segment {i+1}...")
                patternDetection(smoothed_segment)

                # Test Blanks and Samples Relation
                print(f"Testing relation between blanks and samples for segment {i+1}...")
                testRelationBetweenBlanksAndSamples(current_segment)

                # PCA
                print(f"Applying PCA on segment {i+1}...")
                applyPCA(current_segment)

